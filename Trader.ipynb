{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stereifberger/trader_news_tone/blob/main/Trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary"
      ],
      "metadata": {
        "id": "-LQoU2hnCORG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYWGzsadbG5D"
      },
      "source": [
        "## GitHub Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NXPaMbR-2uAY"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/stereifberger/trader_news_tone\"\n",
        "%cd /content/trader_news_tone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "q52pGyzU_HnA"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"sterei@outlook.com\"\n",
        "!git config --global user.name \"stereifberger\"\n",
        "!git add .\n",
        "!git commit -m \"Added saving and loading datasets\"\n",
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3qH1c3ubQhI"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FjsgZ1mFIq9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install alpaca-trade-api\n",
        "!pip install newsapi-python\n",
        "#!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LAO8bhwv2tVb"
      },
      "outputs": [],
      "source": [
        "import alpaca_trade_api as tradeapi\n",
        "from alpaca_trade_api.rest import TimeFrame\n",
        "from alpaca_trade_api.rest import REST\n",
        "from alpaca_trade_api.rest import TimeFrame\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import importlib\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import yfinance as yf\n",
        "import newsapi\n",
        "from newsapi import NewsApiClient\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "from keywords_2 import keywords\n",
        "import numpy as np\n",
        "#import openai\n",
        "#from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTJjk2y5cBZw"
      },
      "source": [
        "## Key, Save and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2WH5eF1b7AC"
      },
      "outputs": [],
      "source": [
        "api = REST('PUBLIC_ALPACA', 'SECRET_ALPACA', 'https://paper-api.alpaca.markets')\n",
        "newsapi = NewsApiClient(api_key='NEWS_API')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mTg29S61MK_h"
      },
      "outputs": [],
      "source": [
        "# File paths to save your data\n",
        "stock_dataframe_path = \"/content/stock_dataframes.pkl\"\n",
        "avaiable_stocks_path = \"avaiable_stocks.pkl\"\n",
        "news_path = 'news.pkl'\n",
        "dataset_path = \"/content/trader_news_tone/dataset.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujHI1-ceJtMx"
      },
      "outputs": [],
      "source": [
        "#Save stock_dataframes (pandas dataframe)\n",
        "#with open(stock_dataframe_path, 'wb') as f:\n",
        "#    pickle.dump(stock_dataframes, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UiG-oolzLfX7"
      },
      "outputs": [],
      "source": [
        "# Load stock_dataframes\n",
        "with open(stock_dataframe_path, 'rb') as f:\n",
        "    stock_dataframes = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News"
      ],
      "metadata": {
        "id": "tRfLSVGmBiOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch News"
      ],
      "metadata": {
        "id": "h_88p77gDOQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsapi_client = NewsApiClient(api_key='KEY')"
      ],
      "metadata": {
        "id": "9MzwAEBS8tqu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_news(num_articles=5, hours_back=24, batch_size=100):\n",
        "    # Define the time window (past 'hours_back' hours)\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(hours=hours_back)\n",
        "\n",
        "    all_articles_list = []\n",
        "    current_start = start_date\n",
        "\n",
        "    while len(all_articles_list) < num_articles:\n",
        "        # Define the current end date for the batch\n",
        "        current_end = min(current_start + timedelta(hours=hours_back / (num_articles // batch_size)), end_date)\n",
        "\n",
        "        # Format the dates as 'YYYY-MM-DDTHH:MM:SS'\n",
        "        current_start_str = current_start.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "        current_end_str = current_end.strftime('%Y-%m-%dT%H:%M:%S')\n",
        "\n",
        "        # Fetch a batch of financial news articles\n",
        "        all_articles = newsapi_client.get_everything(q='finance OR stock',\n",
        "                                              language='en',\n",
        "                                              from_param=current_start_str,\n",
        "                                              to=current_end_str,\n",
        "                                              sort_by='publishedAt',\n",
        "                                              page_size=batch_size)\n",
        "\n",
        "        # Append the batch to the list\n",
        "        all_articles_list.extend(all_articles['articles'])\n",
        "\n",
        "        # Update the start date for the next batch\n",
        "        current_start = current_end\n",
        "\n",
        "        print(f\"Fetched {len(all_articles_list)} articles so far.\")\n",
        "\n",
        "        # Break if we have enough articles\n",
        "        if len(all_articles_list) >= num_articles:\n",
        "            break\n",
        "\n",
        "    # Convert the accumulated articles into a DataFrame\n",
        "    df = pd.DataFrame(all_articles_list)\n",
        "\n",
        "    # Randomly sample 'num_articles' from the pool, without replacement\n",
        "    if len(df) > num_articles:\n",
        "        df = df.sample(n=num_articles, replace=False)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "YExjlZ7EUp9I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUlzQ4ROyQtH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fetch news data\n",
        "news_df = fetch_news(2000, 720, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map to stocks"
      ],
      "metadata": {
        "id": "wH0A_NpUDTW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"Mapping to stocks\")\n",
        "\n",
        "def find_stock(content, keyword_dict):\n",
        "    matched_stocks = []\n",
        "    for stock, keywords in keyword_dict.items():\n",
        "        if any(keyword in content for keyword in keywords):\n",
        "            matched_stocks.append(stock)\n",
        "    return matched_stocks if matched_stocks else []"
      ],
      "metadata": {
        "id": "bDm5IUr06G9Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['stocks'] = news_df['content'].progress_apply(lambda x: find_stock(x, keywords))"
      ],
      "metadata": {
        "id": "fi8_gW6n6MgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embed News"
      ],
      "metadata": {
        "id": "Ofq_eXgCDaUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FinBERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "model = BertModel.from_pretrained('yiyanghkust/finbert-tone')"
      ],
      "metadata": {
        "id": "IaEpvONoB9tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "def generate_news_embeddings(news_df):\n",
        "    tqdm.pandas(desc=\"Generating embeddings\")\n",
        "\n",
        "    # Apply the embedding generation function with progress bar\n",
        "    news_df['embedding'] = news_df['content'].progress_apply(get_embedding)\n",
        "\n",
        "    return news_df"
      ],
      "metadata": {
        "id": "oZYqoTocCvzW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings\n",
        "news_with_embeddings = generate_news_embeddings(news_df)"
      ],
      "metadata": {
        "id": "xZh7scQ3CKxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Keywords"
      ],
      "metadata": {
        "id": "DQ8xZbQiFGod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z1ix3qKJ5lYi"
      },
      "outputs": [],
      "source": [
        "# Replace with your OpenAI API key\n",
        "client = OpenAI(api_key=\"KEY\")\n",
        "def generate_company_keywords(company_names):\n",
        "  prompt = f\"\"\"\n",
        "  Complete my python dictionary for all companies with as many keywords as you can, relevant to that companies market performance:\n",
        "  'Apple': ['apple', 'iphone', 'macbook', 'semiconductor', 'ipad', 'pc', 'microsoft', 'ai', 'china', 'google'],\n",
        "  'Google': ['google', 'search', 'android', 'alphabet', 'apple', 'microsoft', 'openai', 'antitrust'],\n",
        "  'Tesla': ['tesla', 'elon musk', 'electric vehicle', 'model 3', 'bmw', 'car', 'vw'],\n",
        "  \"\"\"\n",
        "  i = 0\n",
        "  asset_junks = []\n",
        "\n",
        "  for name in company_names:\n",
        "    for company in company_names:\n",
        "      prompt += f\"'{company}': ,\\n\"\n",
        "\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  returns = completion.choices[0].message.content\n",
        "  return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ7xAz-p8uEz"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "asset_junks = []\n",
        "\n",
        "while i < len(asset_names):\n",
        "    asset_junks.append([asset_names[i:i+20]])\n",
        "    i += 20\n",
        "\n",
        "for assets in tqdm(asset_junks):\n",
        "  generated_keywords = generate_company_keywords(assets)\n",
        "  with open(f\"keywords.txt\", 'a') as f:\n",
        "    f.write(generated_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Market Data"
      ],
      "metadata": {
        "id": "c43TkA0f1UbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'publishedAt' to datetime format\n",
        "news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt'])\n",
        "\n",
        "# Filter out entries where 'publishedAt' is before 8 AM (hour < 8)\n",
        "news_df = news_df[news_df['publishedAt'].dt.hour >= 8]\n",
        "\n",
        "# Filter out weekends (Saturday=5, Sunday=6)\n",
        "news_df = news_df[~news_df['publishedAt'].dt.weekday.isin([5, 6])]\n"
      ],
      "metadata": {
        "id": "x01DCb1WaxyR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_alpaca_day_data(stock, published_date, timeframe):\n",
        "    start_date = published_date.strftime('%Y-%m-%d')\n",
        "    end_date = (published_date + timedelta(days=1)).strftime('%Y-%m-%d')  # End date is the next day\n",
        "\n",
        "    try:\n",
        "        # Fetch the entire day's data\n",
        "        bars = api.get_bars(stock, timeframe, start=start_date, end=end_date).df\n",
        "\n",
        "        # Check if we received any data\n",
        "        if bars.empty:\n",
        "            #print(f\"No market data for {stock} on {start_date}.\")\n",
        "            return pd.DataFrame()  # Return empty DataFrame if no data\n",
        "\n",
        "        # Ensure the index has the same timezone as the published_time\n",
        "        if bars.index.tz is None:\n",
        "            bars.index = bars.index.tz_localize('UTC')  # Assuming the data is in UTC\n",
        "\n",
        "        return bars\n",
        "\n",
        "    except Exception as e:\n",
        "        #print(f\"Error fetching Alpaca data for {stock}: {e}\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame to skip this stock\n"
      ],
      "metadata": {
        "id": "rAgOCnE8P2Vy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_yahoo_data(stock):\n",
        "    \"\"\"Fetches profit, sales, and EBITDA for the given stock.\"\"\"\n",
        "    stock_data = {}\n",
        "    try:\n",
        "        # Fetch financial data using Yahoo Finance\n",
        "        ticker = yf.Ticker(stock)\n",
        "\n",
        "        # Get the latest financial data\n",
        "        financials = ticker.financials\n",
        "        stock_data['profit'] = financials.loc['Gross Profit'].iloc[0] if 'Gross Profit' in financials.index else None\n",
        "        stock_data['sales'] = financials.loc['Total Revenue'].iloc[0] if 'Total Revenue' in financials.index else None\n",
        "        stock_data['ebitda'] = financials.loc['EBITDA'].iloc[0] if 'EBITDA' in financials.index else None\n",
        "\n",
        "    except Exception as e:\n",
        "        #print(f\"Error fetching Yahoo data for {stock}: {e}\")\n",
        "        stock_data = {'profit': None, 'sales': None, 'ebitda': None}  # Return None for all values on error\n",
        "\n",
        "    return stock_data"
      ],
      "metadata": {
        "id": "Fhp3h7752dyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_market_data(news_df, X, Y, Z):\n",
        "    market_data_dict = {}\n",
        "\n",
        "    for _, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
        "        published_time = pd.to_datetime(row['publishedAt'])\n",
        "\n",
        "        # Ensure the published_time is timezone-aware (assuming it's in UTC)\n",
        "        if published_time.tz is None:\n",
        "            published_time = published_time.tz_localize('UTC')\n",
        "\n",
        "        published_date = published_time.date()\n",
        "        stock_list = row['stocks']\n",
        "        embedding = row['embedding']  # Get the embedding for this news entry\n",
        "\n",
        "        # Ensure that stock_list is a list; if it's a string, convert it to a list by splitting on commas\n",
        "        if isinstance(stock_list, str):\n",
        "            stock_list = stock_list.strip(\"[]\").replace(\"'\", \"\").split(\", \")\n",
        "\n",
        "        for stock in stock_list:\n",
        "            stock = stock.strip()\n",
        "\n",
        "            # Fetch Alpaca data for the entire publication day\n",
        "            alpaca_day_data = fetch_alpaca_day_data(stock, published_time, tradeapi.TimeFrame.Minute)\n",
        "\n",
        "            # Skip if the Alpaca data is empty\n",
        "            if alpaca_day_data.empty:\n",
        "                #print(f\"No market data for {stock} on the given day. Skipping to the next stock.\")\n",
        "                continue\n",
        "\n",
        "            # Filter the X minutes of data before the published time\n",
        "            filtered_data = alpaca_day_data[\n",
        "                (alpaca_day_data.index >= published_time - timedelta(minutes=X)) &\n",
        "                (alpaca_day_data.index <= published_time + timedelta(minutes=Y))\n",
        "            ]\n",
        "\n",
        "\n",
        "            # Skip if we don't have enough data in the X-minute window\n",
        "            if filtered_data.empty:\n",
        "                #print(f\"Not enough data for {stock} within the X-minute window. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Fetch Yahoo Finance data\n",
        "            yahoo_data = fetch_yahoo_data(stock)\n",
        "\n",
        "            # Create a list to store rows for the current stock\n",
        "            current_stock_data = []\n",
        "\n",
        "            # Create rows for each minute of data within the X minutes\n",
        "            for i in range(len(filtered_data[:-Y])):\n",
        "                # Create a dictionary for each minute's data\n",
        "                row_data = {\n",
        "                    'stock': stock,\n",
        "                    'publishedAt': row['publishedAt'],\n",
        "                    'open': filtered_data.iloc[i]['open'],\n",
        "                    'high': filtered_data.iloc[i]['high'],\n",
        "                    'low': filtered_data.iloc[i]['low'],\n",
        "                    'close': filtered_data.iloc[i]['close'],\n",
        "                    'volume': filtered_data.iloc[i]['volume'],\n",
        "                    'profit': yahoo_data['profit'],\n",
        "                    'sales': yahoo_data['sales'],\n",
        "                    'ebitda': yahoo_data['ebitda'],\n",
        "                    'embedding': embedding\n",
        "                }\n",
        "                current_stock_data.append(row_data)\n",
        "\n",
        "            if len(filtered_data) >= X + Y:\n",
        "                # Generate the future price and target based on price change\n",
        "                last_time = filtered_data.close.iloc[-Y]\n",
        "                future_time = filtered_data.close.iloc[-1]\n",
        "                price_increase = future_time - last_time / last_time * 100\n",
        "\n",
        "                # Add a final row with NaN values except for 'target'\n",
        "                final_row = {\n",
        "                    'stock': stock,\n",
        "                    'publishedAt': row['publishedAt'],\n",
        "                    'open': None,\n",
        "                    'high': None,\n",
        "                    'low': None,\n",
        "                    'close': None,\n",
        "                    'volume': None,\n",
        "                    'profit': None,\n",
        "                    'sales': None,\n",
        "                    'ebitda': None,\n",
        "                    'embedding': None,\n",
        "                    'target': (price_increase > Z).astype(int)\n",
        "                }\n",
        "                current_stock_data.append(final_row)\n",
        "                # Convert the current stock's data to a DataFrame\n",
        "                stock_df = pd.DataFrame(current_stock_data)\n",
        "\n",
        "                # Store the DataFrame in the dictionary with key as (stock, publishedAt)\n",
        "                market_data_dict[(stock, published_time)] = stock_df\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    return market_data_dict\n",
        "\n",
        "# Example usage\n",
        "X = 30  # Last X minutes of stock data\n",
        "Y = 20  # Y minutes after the last market data to check price increase\n",
        "Z = 1   # Z percent price increase threshold\n",
        "\n",
        "# Assuming news_df is already loaded with relevant columns\n",
        "market_data_dict = generate_market_data(news_df.head(100), X, Y, Z)"
      ],
      "metadata": {
        "id": "mh59djqTBeZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save stock_dataframes\n",
        "with open(dataset_path, 'wb') as f:\n",
        "    pickle.dump(market_data_dict, f)"
      ],
      "metadata": {
        "id": "_EiSIHNMJaSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stock_dataframes\n",
        "with open(dataset_path, 'rb') as f:\n",
        "    market_data_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "Q5S-t0FRs1RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum length for padding\n",
        "max_embedding_length = 128  # Set this to your desired length\n",
        "\n",
        "# Prepare the dataset\n",
        "data = []\n",
        "\n",
        "# Traverse through the dictionary to process each dataframe\n",
        "for (stock, timestamp), df in market_data_dict.items():\n",
        "    # Ensure the dataframe is sorted by time if necessary\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Determine the size of the series (assume you're using the first n-1 indices for features)\n",
        "    feature_end_index = len(df)-2\n",
        "    target_index = len(df)-1  # Assume index 3 is where n+1 target value lies\n",
        "\n",
        "    for idx in range(len(df) - target_index):\n",
        "        # Slice the relevant series for each feature up to n-1\n",
        "        open_series = df.iloc[idx:feature_end_index]['open'].tolist()\n",
        "        high_series = df.iloc[idx:feature_end_index]['high'].tolist()\n",
        "        low_series = df.iloc[idx:feature_end_index]['low'].tolist()\n",
        "        close_series = df.iloc[idx:feature_end_index]['close'].tolist()\n",
        "        volume_series = df.iloc[idx:feature_end_index]['volume'].tolist()\n",
        "        profit_series = df.iloc[idx:feature_end_index]['profit'].tolist()\n",
        "        sales_series = df.iloc[idx:feature_end_index]['sales'].tolist()\n",
        "        ebitda_series = df.iloc[idx:feature_end_index]['ebitda'].tolist()\n",
        "\n",
        "        # Fetch the target for n+1\n",
        "        target_value = df.iloc[target_index]['target']\n",
        "\n",
        "        # Get embedding vector for last reliable time step (assume last in series for simplicity)\n",
        "        embedding_vector = df.iloc[feature_end_index]['embedding']\n",
        "\n",
        "        # Only consider rows with proper sequence and no missing embedding\n",
        "        if embedding_vector is not None:\n",
        "            # Convert embedding to a numpy array and pad it\n",
        "            embedding_array = np.array(embedding_vector)\n",
        "            # Pad the embedding array to max_embedding_length\n",
        "            padded_embedding = np.pad(embedding_array, (0, max(0, max_embedding_length - len(embedding_array))), 'constant')\n",
        "\n",
        "            # Add to data list\n",
        "            data.append({\n",
        "                'stock_under_timestamp': f\"{stock}_{timestamp}\",\n",
        "                'open': open_series,\n",
        "                'high': high_series,\n",
        "                'low': low_series,\n",
        "                'close': close_series,\n",
        "                'volume': volume_series,\n",
        "                'profit': profit_series,\n",
        "                'sales': sales_series,\n",
        "                'ebitda': ebitda_series,\n",
        "                'embedding': padded_embedding,  # Use padded embedding\n",
        "                'target': target_value\n",
        "            })\n",
        "\n",
        "# Creating DataFrame\n",
        "final_df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "U8esKRskbRju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "AfAKmeFoeiBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.drop('stock_under_timestamp', axis=1)"
      ],
      "metadata": {
        "id": "lo3cclrrdtF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_vector_column(df, column_name, max_length):\n",
        "  \"\"\"Pads vectors in a DataFrame column to a specified length.\"\"\"\n",
        "  padded_vectors = []\n",
        "  for vector in df[column_name]:\n",
        "    if len(vector) < max_length:\n",
        "      padded_vector = np.pad(vector, (0, max_length - len(vector)), 'constant')\n",
        "    else:\n",
        "      padded_vector = vector[:max_length]\n",
        "    padded_vectors.append(padded_vector)\n",
        "  return padded_vectors\n",
        "\n",
        "# Assuming 'embedding' column contains vectors\n",
        "max_embedding_length = final_df['embedding'].apply(len).max()  # Find the maximum length of embedding\n",
        "\n",
        "# Pad the 'open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda' vectors\n",
        "columns_to_pad = ['open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda']\n",
        "for column in columns_to_pad:\n",
        "    final_df[column] = pad_vector_column(final_df, column, max_embedding_length)"
      ],
      "metadata": {
        "id": "6ZMgjHcmgQKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_nan_none_with_zero(df, columns_to_process):\n",
        "    for column in columns_to_process:\n",
        "        df[column] = df[column].apply(lambda vector: [0 if pd.isnull(x) or x is None else x for x in vector])\n",
        "    return df\n",
        "\n",
        "columns_to_process = ['open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda']\n",
        "final_df = replace_nan_none_with_zero(final_df, columns_to_process)\n"
      ],
      "metadata": {
        "id": "qayKuxIQgare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = final_df"
      ],
      "metadata": {
        "id": "8A3RvVhQiXb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Iterate through the columns to normalize\n",
        "for column in ['open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda']:\n",
        "  # Flatten the list of vectors into a single array\n",
        "  all_values = [value for sublist in new[column] for value in sublist]\n",
        "\n",
        "  # Reshape the array to fit the scaler\n",
        "  all_values_reshaped = np.array(all_values).reshape(-1, 1)\n",
        "\n",
        "  # Fit and transform the scaler on all values in the column\n",
        "  scaled_values = scaler.fit_transform(all_values_reshaped)\n",
        "\n",
        "  # Reshape back to the original list of vectors\n",
        "  new_values = []\n",
        "  start_index = 0\n",
        "  for vector in new[column]:\n",
        "    end_index = start_index + len(vector)\n",
        "    new_values.append(scaled_values[start_index:end_index].flatten().tolist())\n",
        "    start_index = end_index\n",
        "\n",
        "  # Replace the original column with the scaled values\n",
        "  new[column] = new_values\n",
        "\n"
      ],
      "metadata": {
        "id": "_tKjInz5TukV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_vector_column(df, column_name):\n",
        "  \"\"\"Normalizes all values within a vector column between 0 and 1.\"\"\"\n",
        "  all_values = []\n",
        "  for vector in df[column_name]:\n",
        "    all_values.extend(vector)\n",
        "\n",
        "  min_val = np.min(all_values)\n",
        "  max_val = np.max(all_values)\n",
        "\n",
        "  normalized_vectors = []\n",
        "  for vector in df[column_name]:\n",
        "    normalized_vector = [(x - min_val) / (max_val - min_val) if max_val - min_val != 0 else 0 for x in vector]\n",
        "    normalized_vectors.append(normalized_vector)\n",
        "  return normalized_vectors\n",
        "\n",
        "# Normalize the 'open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda' vectors\n",
        "columns_to_normalize = ['open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda']\n",
        "for column in columns_to_normalize:\n",
        "  new[column] = normalize_vector_column(new, column)"
      ],
      "metadata": {
        "id": "DSRqHEbsg1f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_vector_to_long(df, columns_to_convert):\n",
        "  \"\"\"Converts all vector values in the specified columns to long.\"\"\"\n",
        "  for column in columns_to_convert:\n",
        "    df[column] = df[column].apply(lambda vector: [int(x) for x in vector])\n",
        "  return df\n",
        "\n",
        "columns_to_convert = ['open', 'high', 'low', 'close', 'volume', 'profit', 'sales', 'ebitda', embedding]\n",
        "new = convert_vector_to_long(new, columns_to_convert)\n"
      ],
      "metadata": {
        "id": "ZvW-GGJ9gMRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy3F9zsZfDRD"
      },
      "source": [
        "## Get active Assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr9m_QSKh9nz"
      },
      "outputs": [],
      "source": [
        "active_assets = api.list_assets(status='active')  # you could leave out the status to also get the inactive ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvZQdHWe4pDX"
      },
      "outputs": [],
      "source": [
        "asset_names = [asset.name for asset in active_assets]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOGf_Q6teLCE"
      },
      "source": [
        "# Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYd3d0cFjN86"
      },
      "outputs": [],
      "source": [
        "class AsymmetricBCEWithLogitsLoss(nn.Module):\n",
        "    def __init__(self, pos_weight=None, alpha=1.0):\n",
        "        super(AsymmetricBCEWithLogitsLoss, self).__init__()\n",
        "        self.pos_weight = pos_weight\n",
        "        self.alpha = alpha  # Alpha is the penalty factor for false positives\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Compute standard BCE loss\n",
        "        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, pos_weight=self.pos_weight, reduction='none')\n",
        "\n",
        "        # Compute probabilities using sigmoid\n",
        "        probs = torch.sigmoid(inputs)\n",
        "\n",
        "        # Modify the loss for false positives (when true label is 0 but predicted 1)\n",
        "        modulated_loss = torch.where(targets == 0, self.alpha * bce_loss, bce_loss)  # Apply alpha only to false positives\n",
        "\n",
        "        return modulated_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJgMPJiAmMOA"
      },
      "outputs": [],
      "source": [
        "def calculate_confirmatory_factor(y_true, y_pred, predictions, precision):\n",
        "    # Compute base probability of 1's (Prior P(H))\n",
        "    base_prob = (y_true == 1).float().mean().item()  # proportion of 1s in the true labels\n",
        "\n",
        "    # Calculate the Confirmatory Factor (CF)\n",
        "    if base_prob > 0:  # Avoid division by zero\n",
        "        confirmatory_factor = precision / base_prob\n",
        "    else:\n",
        "        confirmatory_factor = float('inf')  # If no 1's in the dataset\n",
        "\n",
        "    return base_prob, confirmatory_factor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4q0TLUAcOi4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StockDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        # Prepare the training input (convert list to tensor)\n",
        "        market_data = [torch.tensor(row['open'], dtype=torch.float),\n",
        "                       torch.tensor(row['high'], dtype=torch.float),\n",
        "                       torch.tensor(row['low'], dtype=torch.float),\n",
        "                       torch.tensor(row['profit'], dtype=torch.float),\n",
        "                       torch.tensor(row['sales'], dtype=torch.float),\n",
        "                       torch.tensor(row['ebitda'], dtype=torch.float)]\n",
        "\n",
        "        # Flatten the list of tensors into a single tensor for input\n",
        "        input_tensor = torch.cat(market_data + [torch.tensor(row['embedding'])])\n",
        "\n",
        "        # Prepare the target value as a tensor\n",
        "        target_tensor = torch.tensor(row['target'], dtype=torch.float)\n",
        "\n",
        "        # Return a dictionary of inputs and target\n",
        "        return {'input': input_tensor, 'target': target_tensor}\n",
        "\n",
        "# Create your dataset\n",
        "stock_dataset = StockDataset(final_df)\n",
        "\n",
        "# Create a DataLoader instance\n",
        "batch_size = 16  # or any size you prefer\n",
        "stock_dataloader = DataLoader(stock_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over DataLoader\n",
        "for batch in stock_dataloader:\n",
        "    inputs = batch['input']  # batched inputs\n",
        "    targets = batch['target']  # batched targets\n",
        "\n",
        "    # Now you can pass these inputs and targets to your model\n",
        "    # e.g., outputs = model(inputs)"
      ],
      "metadata": {
        "id": "Kl_CYHXwOxkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "7hnM8mq88BiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(stock_dataframes, X, Y, Z, model_type, epochs, batch_size, learning_rate, architecture_params):\n",
        "    # Prepare data\n",
        "    data = stock_dataframes.to_numpy()\n",
        "\n",
        "    features = data[:, 1:-1]  # All columns except the first (if it is an index) and last (target)\n",
        "    targets = data[:, -1]     # Last column as the target\n",
        "\n",
        "    # Split the data into features and targets\n",
        "    features, targets = [], []\n",
        "    for stock_data in data:\n",
        "        if len(stock_data.shape) == 1:\n",
        "            # If stock_data is a single row (vector), separate the features and target manually\n",
        "            stock_features = stock_data[:-1]  # All elements except the last\n",
        "            stock_target = stock_data[-1]     # The last element is the target\n",
        "        else:\n",
        "            # Normal case where stock_data has multiple rows\n",
        "            stock_features = stock_data[:-1]  # All rows except the last\n",
        "            stock_target = stock_data[-1][-1]  # Last row, last element is the target\n",
        "\n",
        "        features.append(stock_features)\n",
        "        targets.append(stock_target)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.tensor(features, dtype=torch.float32)\n",
        "    y_train = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)  # Add dimension for binary classification\n",
        "    X_test = torch.tensor(features, dtype=torch.float32)\n",
        "    y_test = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)  # Add dimension for binary classification\n",
        "\n",
        "    # Create DataLoader for batching\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    # Initialize model\n",
        "    #model = get_model(model_type, **architecture_params)\n",
        "    model = get_model(\n",
        "    model_type,\n",
        "    encoder_params=architecture_params['encoder_params'],\n",
        "    decoder_params=architecture_params['decoder_params'],\n",
        "    device=device\n",
        ")\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Calculate the positive class weight for imbalanced datasets\n",
        "    num_ones = (y_train == 1).sum().item()\n",
        "    num_zeros = (y_train == 0).sum().item()\n",
        "    pos_weight = torch.tensor([num_zeros / num_ones], dtype=torch.float32).to(device)\n",
        "\n",
        "    # Use BCEWithLogitsLoss with pos_weight\n",
        "    criterion = AsymmetricBCEWithLogitsLoss(pos_weight=pos_weight, alpha=1.0)  # Alpha penalizes false positives\n",
        "    # If your model's final layer does not include sigmoid, use this instead:\n",
        "    #criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # Training loop with batching\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X, 1)\n",
        "\n",
        "            # Ensure the output has the same shape as the target by squeezing the extra dimension\n",
        "\n",
        "            outputs = torch.squeeze(outputs, dim=-1)  # Squeeze the last dimension (which is 1)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}')\n",
        "\n",
        "    # Evaluation on test set\n",
        "    #model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(X_test, 1)\n",
        "        #test_outputs = torch.sigmoid(test_outputs)\n",
        "        # Ensure the output has the same shape as the target by squeezing the extra dimension\n",
        "        test_outputs = torch.squeeze(test_outputs, dim=-1)  # Squeeze the last dimension (which is 1)\n",
        "        test_loss = criterion(test_outputs, y_test).item()\n",
        "\n",
        "        # Binarize predictions with a threshold of 0.5\n",
        "        threshold = 0.5\n",
        "        predictions = (test_outputs > threshold).float()\n",
        "        print(test_outputs[:20])\n",
        "\n",
        "        precision = precision_score(y_test.cpu(), predictions.cpu())\n",
        "        recall = recall_score(y_test.cpu(), predictions.cpu())\n",
        "        f1 = f1_score(y_test.cpu(), predictions.cpu())\n",
        "        accuracy = ((predictions == y_test).float().mean()).item()\n",
        "\n",
        "        # Calculate additional metrics (base_prob, confirmatory_factor)\n",
        "        base_prob, confirmatory_factor = calculate_confirmatory_factor(y_test, test_outputs, predictions, precision)\n",
        "\n",
        "    # Cleanup\n",
        "    del model, optimizer, criterion\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return test_loss, accuracy, precision, recall, f1, base_prob, confirmatory_factor\n",
        "\n",
        "# Define parameter grid for grid search\n",
        "#param_grid = {\n",
        "#    'X': [10],  # Look-back window sizes\n",
        "#    'Y': [5],  # Prediction horizon\n",
        "#    'Z': [1],  # Price increase thresholds\n",
        "#    'model_type': ['transformer'],  # Model types\n",
        "#    'epochs': [10],\n",
        "#    'batch_size': 64,\n",
        "#    'learning_rate': 0.001,\n",
        "#    'architecture': [{'input_dim': 768, 'num_heads': 2, 'num_layers': 2, 'output_dim': 1}]\n",
        "#}\n",
        "param_grid = {\n",
        "    'X': [10],  # Look-back window sizes\n",
        "    'Y': [5],   # Prediction horizon\n",
        "    'Z': [1],   # Price increase thresholds\n",
        "    'model_type': ['transformer'],  # Model types\n",
        "    'epochs': [20],\n",
        "    'batch_size': 128,\n",
        "    'learning_rate': 0.001,\n",
        "    'encoder_params': [{'input_dim': 768, 'emb_dim': 240, 'num_heads': 8, 'hidden_dim': 2048, 'num_layers': 6, 'dropout': 0}],\n",
        "    'decoder_params': [{'output_dim': 1, 'emb_dim': 240, 'num_heads': 8, 'hidden_dim': 2048, 'num_layers': 6}]\n",
        "}\n",
        "\n",
        "# Run grid search\n",
        "grid_search(new, param_grid)"
      ],
      "metadata": {
        "id": "Xk57V2sJ_qh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCbQC41-bePw"
      },
      "outputs": [],
      "source": [
        "def grid_search(stock_dataframes, param_grid):\n",
        "    results = []\n",
        "\n",
        "    for X in param_grid['X']:\n",
        "        for Y in param_grid['Y']:\n",
        "            for Z in param_grid['Z']:\n",
        "                for model_type in param_grid['model_type']:\n",
        "                    for epochs in param_grid['epochs']:\n",
        "                        for architecture_params in param_grid['architecture']:\n",
        "                            test_loss, accuracy, precision, recall, f1, base_prob, confirmatory_factor = train_and_evaluate(\n",
        "                                stock_dataframes, X, Y, Z, model_type, epochs,\n",
        "                                param_grid['batch_size'], param_grid['learning_rate'], architecture_params\n",
        "                            )\n",
        "                            result = {\n",
        "                                'confirmatory_factor': confirmatory_factor,\n",
        "                                'precision': precision,\n",
        "                                'recall': recall,\n",
        "                                'base_prob': base_prob,\n",
        "                                'f1': f1,\n",
        "                                'accuracy': accuracy,\n",
        "                                'X': X,\n",
        "                                'Y': Y,\n",
        "                                'Z': Z,\n",
        "                                'model_type': model_type,\n",
        "                                'epochs': epochs,\n",
        "                                'test_loss': test_loss\n",
        "                            }\n",
        "                            results.append(result)\n",
        "                            print(result)\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_csv('training_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(stock_dataframes, param_grid):\n",
        "    results = []\n",
        "\n",
        "    for X in param_grid['X']:\n",
        "        for Y in param_grid['Y']:\n",
        "            for Z in param_grid['Z']:\n",
        "                for model_type in param_grid['model_type']:\n",
        "                    for epochs in param_grid['epochs']:\n",
        "                        for encoder_params in param_grid['encoder_params']:\n",
        "                            for decoder_params in param_grid['decoder_params']:\n",
        "                                test_loss, accuracy, precision, recall, f1, base_prob, confirmatory_factor = train_and_evaluate(\n",
        "                                    stock_dataframes, X, Y, Z, model_type, epochs,\n",
        "                                    param_grid['batch_size'], param_grid['learning_rate'],\n",
        "                                    {'encoder_params': encoder_params, 'decoder_params': decoder_params}\n",
        "                                )\n",
        "                                result = {\n",
        "                                    'confirmatory_factor': confirmatory_factor,\n",
        "                                    'precision': precision,\n",
        "                                    'recall': recall,\n",
        "                                    'base_prob': base_prob,\n",
        "                                    'f1': f1,\n",
        "                                    'accuracy': accuracy,\n",
        "                                    'X': X,\n",
        "                                    'Y': Y,\n",
        "                                    'Z': Z,\n",
        "                                    'model_type': model_type,\n",
        "                                    'epochs': epochs,\n",
        "                                    'test_loss': test_loss\n",
        "                                }\n",
        "                                results.append(result)\n",
        "                                print(result)\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_results.to_csv('training_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NueJ6pI01xCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddapK-vRemSg"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB0ipROnautn"
      },
      "outputs": [],
      "source": [
        "class LSTMEncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTMEncoderDecoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        # No sigmoid here\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        output, _ = self.decoder(hidden[-1].unsqueeze(0))\n",
        "        output = self.fc(output.squeeze(0))  # No sigmoid here\n",
        "        return output  # Return raw logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i6bHHa0EfK8"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, num_layers, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [sequence_length, batch_size, input_dim]\n",
        "        x = self.encoder(x)  # Pass the entire sequence through the encoder\n",
        "\n",
        "        # Select the output from the last time step\n",
        "        x = x[:, -1, :]  # Shape: [batch_size, input_dim]\n",
        "\n",
        "        # Apply the fully connected layer and sigmoid activation\n",
        "        x = self.fc(x)  # Shape: [batch_size, output_dim]\n",
        "        return self.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaBlKYmzboG3"
      },
      "outputs": [],
      "source": [
        "def get_model(model_type, **kwargs):\n",
        "    if model_type == 'lstm':\n",
        "        return LSTMEncoderDecoder(**kwargs)\n",
        "    elif model_type == 'transformer':\n",
        "        return TransformerModel(**kwargs)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type: Choose either 'lstm' or 'transformer'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_type, encoder_params, decoder_params, device):\n",
        "    if model_type == 'lstm':\n",
        "        return LSTMEncoderDecoder(**encoder_params)  # LSTM architecture (leave as is if using this option)\n",
        "    elif model_type == 'transformer':\n",
        "        encoder = TransformerEncoder(**encoder_params)  # Pass the encoder params\n",
        "        decoder = TransformerDecoder(**decoder_params)  # Pass the decoder params\n",
        "        return Seq2SeqTransformer(encoder, decoder, device)  # Use your new Transformer model\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type: Choose either 'lstm' or 'transformer'\")"
      ],
      "metadata": {
        "id": "abJCqwAr0o3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.linear_input = nn.Linear(input_dim, emb_dim)  # Linear layer for continuous features\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.linear_input(src)  # Apply the linear layer instead of embedding\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_encoder(embedded)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, num_heads, hidden_dim, num_layers, max_seq_len=100):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        embedded = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :].to(tgt.device)\n",
        "        embedded = embedded.permute(1, 0, 2)  # [seq_len, batch_size, emb_dim]\n",
        "        output = self.transformer_decoder(embedded, memory)\n",
        "        output = output.permute(1, 0, 2)  # [batch_size, seq_len, emb_dim]\n",
        "        prediction = self.fc_out(output)\n",
        "        return prediction\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg_len):\n",
        "        memory = self.encoder(src)\n",
        "        tgt = torch.zeros((src.size(0), trg_len), device=self.device, dtype=torch.long)\n",
        "        output = self.decoder(tgt, memory)\n",
        "        return output"
      ],
      "metadata": {
        "id": "NKBY0OGDzD1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_tra = TransformerEncoder(, 150, 5, 150, 1, dropout=0.1)\n",
        "decoder_tra = TransformerDecoder(14, 150, 1, 150, 3)\n",
        "tra_ed_model = Seq2SeqTransformer(encoder_tra, decoder_tra, device)"
      ],
      "metadata": {
        "id": "7PYpkh2ez34h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxBy-KCNfMIW"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gFSVDHVi9mGp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "bef259fe-3740-432f-aad2-73b1c7adcae9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-375-01ca551e4493>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Run grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-317-7efa89997f77>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(stock_dataframes, param_grid)\u001b[0m\n\u001b[1;32m     10\u001b[0m                             test_loss, accuracy, precision, recall, f1, base_prob, confirmatory_factor = train_and_evaluate(\n\u001b[1;32m     11\u001b[0m                                 \u001b[0mstock_dataframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                 \u001b[0mparam_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                             )\n\u001b[1;32m     14\u001b[0m                             result = {\n",
            "\u001b[0;31mKeyError\u001b[0m: 'size'"
          ]
        }
      ],
      "source": [
        "# Define parameter grid for grid search\n",
        "param_grid = {\n",
        "    'X': [10],  # Look-back window sizes\n",
        "    'Y': [5],  # Prediction horizon\n",
        "    'Z': [1],  # Price increase thresholds\n",
        "    'model_type': ['transformer'],  # Model types\n",
        "    'epochs': [40],\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'architecture': [{'input_dim': 5, 'num_heads': 5, 'num_layers': 2, 'output_dim': 1}]\n",
        "}\n",
        "\n",
        "# Run grid search\n",
        "grid_search(final_df, param_grid)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}